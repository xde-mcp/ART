{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 18:27:21 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading '.gitattributes' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b\n",
      "Downloading 'README.md' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/bc5f30d6632ac0efdc7be2e9095e9e9579af2e33.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/bc5f30d6632ac0efdc7be2e9095e9e9579af2e33\n",
      "Downloading 'added_tokens.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed\n",
      "Downloading 'chat_template.jinja' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/bdf7919a96cfe43d50914a007b9c0877bd0ec27e.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/bdf7919a96cfe43d50914a007b9c0877bd0ec27e\n",
      "Downloading 'config.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/206ceb93a00489bf597e91de392b40eef332f6fc.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/206ceb93a00489bf597e91de392b40eef332f6fc\n",
      "Downloading 'generation_config.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/aae4bea99feafeeb5526928b89ce09695bc6eb08.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/aae4bea99feafeeb5526928b89ce09695bc6eb08\n",
      "Downloading 'merges.txt' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7\n",
      "Downloading 'model-00001-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/e5da875a02c8eb8d139eea2e3a3822a1d8228036724ea76ff2fc1dfc2c8f329a.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/e5da875a02c8eb8d139eea2e3a3822a1d8228036724ea76ff2fc1dfc2c8f329a\n",
      "Downloading 'model-00002-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/007c079c15cae8ba9acd0104b61b68d9b49ff2631f4ea672e0821e87ac0aa3a7.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/007c079c15cae8ba9acd0104b61b68d9b49ff2631f4ea672e0821e87ac0aa3a7\n",
      "Downloading 'model-00003-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/56e2b3a80f934c890e5afd7c7a1512b48910f2f45acdc125c370716394c90313.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/56e2b3a80f934c890e5afd7c7a1512b48910f2f45acdc125c370716394c90313\n",
      "Downloading 'model-00004-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/c8eceb24e1e8cefae1f13e91834ea13ee4e6433624834f9565a1c0f99506e5c9.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/c8eceb24e1e8cefae1f13e91834ea13ee4e6433624834f9565a1c0f99506e5c9\n",
      "Downloading 'model-00005-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ff78ade9e51143cb64f6ec704faa2b1e5d409f8db01cd6f35976999029f30898.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ff78ade9e51143cb64f6ec704faa2b1e5d409f8db01cd6f35976999029f30898\n",
      "Downloading 'model-00006-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/f951031c7c5b579c6bb82ee01887f0abede152768783a0d7d8eaa293622c6672.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/f951031c7c5b579c6bb82ee01887f0abede152768783a0d7d8eaa293622c6672\n",
      "Downloading 'model-00007-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/62dba33c497528a50fcd5c61d137a8f939311084fb08cb42890cf057095692db.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/62dba33c497528a50fcd5c61d137a8f939311084fb08cb42890cf057095692db\n",
      "Downloading 'model-00008-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/b49614615e7c280726aee39e8e4020574c6250337466b0a182e0c941274d9a9a.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/b49614615e7c280726aee39e8e4020574c6250337466b0a182e0c941274d9a9a\n",
      "Downloading 'model-00009-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/3c76bfbd34b631879d8c926d923fc718a533e2dc4c9b1dc0e4903663993d8728.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/3c76bfbd34b631879d8c926d923fc718a533e2dc4c9b1dc0e4903663993d8728\n",
      "Downloading 'model-00010-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/c2cfea0d51e4d1dc78b55e65b21636957bf9c201137c48697a520dd3d28e5551.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/c2cfea0d51e4d1dc78b55e65b21636957bf9c201137c48697a520dd3d28e5551\n",
      "Downloading 'model-00011-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/397f63feb7cf3bba972fc309a7640a98223e66d51c55558c1dba3c4ed45ed892.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/397f63feb7cf3bba972fc309a7640a98223e66d51c55558c1dba3c4ed45ed892\n",
      "Downloading 'model-00012-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/1dad948134cc8b8b834899699d44546c7d142ff4d5d8354fa427c18a437cd1b6.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/1dad948134cc8b8b834899699d44546c7d142ff4d5d8354fa427c18a437cd1b6\n",
      "Downloading 'model-00013-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/a504a2fa85eb4618ef42c5be610f5429c1fc13ad06ac164bc39ff620d19ee8c2.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/a504a2fa85eb4618ef42c5be610f5429c1fc13ad06ac164bc39ff620d19ee8c2\n",
      "Downloading 'model-00014-of-00014.safetensors' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ae5c47ee8b511f3d2af0428de193f86a905ebcd47d1b4f348b94ccbc706e721d.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ae5c47ee8b511f3d2af0428de193f86a905ebcd47d1b4f348b94ccbc706e721d\n",
      "Downloading 'model.safetensors.index.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/9c12a5bc60a45cefc1f7bd9a03456ab6d20c2174.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/9c12a5bc60a45cefc1f7bd9a03456ab6d20c2174\n",
      "Downloading 'special_tokens_map.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ac23c0aaa2434523c494330aeb79c58395378103.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ac23c0aaa2434523c494330aeb79c58395378103\n",
      "Downloading 'tokenizer.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4\n",
      "Downloading 'tokenizer_config.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ddaf69808214a44fdd26d3785b66c1367c78277a.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/ddaf69808214a44fdd26d3785b66c1367c78277a\n",
      "Downloading 'vocab.json' to '/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n",
      "Download complete. Moving file to /home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/blobs/4783fe10ac3adce15ac8f358ef5462739852c569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sky/.cache/huggingface/hub/models--willcb--Qwen3-32B/snapshots/4371efdac7e15a639136f218777f6bf3730d1ad2\n",
      "INFO 07-10 18:28:35 [config.py:823] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-10 18:28:35 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 07-10 18:28:35 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 07-10 18:28:36 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-10 18:28:36 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='willcb/Qwen3-32B', speculative_config=None, tokenizer='willcb/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=willcb/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-10 18:28:36 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_e0997232'), local_subscribe_addr='ipc:///tmp/d1aff72b-c00e-4332-8ec2-1746cfb4b0d2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab9a50>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_252a4bea'), local_subscribe_addr='ipc:///tmp/cfbcfa60-ec15-447a-a83d-13accfc4db6f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab8b20>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d7be156f'), local_subscribe_addr='ipc:///tmp/2bf11dc5-68fc-4eb3-ae36-4f640d5fcb32', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab8940>\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1f8aeffd'), local_subscribe_addr='ipc:///tmp/cdb57267-ea25-400f-bf6e-5289f8d4263b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab8250>\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b37d5ffc'), local_subscribe_addr='ipc:///tmp/b7875885-6839-4121-b536-fb8c12e84cb5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab92a0>\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_856a19e6'), local_subscribe_addr='ipc:///tmp/39492bc0-4827-4ead-83f5-12f846753d31', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 07-10 18:28:36 [worker_base.py:590] Injected <class 'art.vllm.engine.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run', 'time']\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab9420>\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_07529882'), local_subscribe_addr='ipc:///tmp/a17579da-964f-4183-800b-b9f93bbc08a3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab9630>\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_10729a81'), local_subscribe_addr='ipc:///tmp/94c29a67-d8df-4895-a6d6-10e61d7dc57a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 18:28:36 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79e272ab9c60>\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_11a6fe30'), local_subscribe_addr='ipc:///tmp/7829518e-99b9-4b82-80e9-d39974f6eb6d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W710 18:28:46.726363006 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.816596278 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.816955430 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.976976145 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.977255207 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.124608631 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.125017404 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.641564419 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.641975251 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.684606516 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.686093893 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.733199500 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:47.733529884 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:48.776077801 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:48.776291606 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:48.777544266 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W710 18:28:48.820161000 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:48 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-10 18:28:48 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 18:28:53 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_e2a537e4'), local_subscribe_addr='ipc:///tmp/825508b3-1e36-4abd-ace8-0edd7eb7a392', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:53 [parallel_state.py:1065] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:53 [parallel_state.py:1065] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "INFO 07-10 18:28:53 [parallel_state.py:1065] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-10 18:28:53 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1595] Starting to load model willcb/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:53 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:53 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:53 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2b691a85e74f02a18d4cddecd6cb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:56 [default_loader.py:272] Loading weights took 2.84 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:56 [default_loader.py:272] Loading weights took 2.84 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:56 [default_loader.py:272] Loading weights took 2.99 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:57 [default_loader.py:272] Loading weights took 2.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:57 [default_loader.py:272] Loading weights took 2.80 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:57 [default_loader.py:272] Loading weights took 2.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:57 [default_loader.py:272] Loading weights took 3.40 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.295183 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.346557 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.393145 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.422566 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.470878 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:57 [default_loader.py:272] Loading weights took 3.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.752320 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.664498 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:28:57 [gpu_model_runner.py:1624] Model loading took 7.6871 GiB and 3.929632 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_6_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.09 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.10 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_5_0 for vLLM's torch.compile\n",
      "INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.18 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.17 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.20 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.24 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_4_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.22 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:462] Using cache directory: /home/sky/.cache/vllm/torch_compile_cache/af2be87ed6/rank_7_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:29:09 [backends.py:472] Dynamo bytecode transform time: 11.35 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:29:13 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:29:55 [backends.py:173] Compiling a graph for general shape takes 45.51 s\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:29:55 [backends.py:173] Compiling a graph for general shape takes 46.03 s\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:29:55 [backends.py:173] Compiling a graph for general shape takes 45.86 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:29:56 [backends.py:173] Compiling a graph for general shape takes 46.80 s\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:29:57 [backends.py:173] Compiling a graph for general shape takes 47.14 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:29:57 [backends.py:173] Compiling a graph for general shape takes 47.30 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:29:57 [backends.py:173] Compiling a graph for general shape takes 47.29 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:29:57 [backends.py:173] Compiling a graph for general shape takes 47.50 s\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 56.60 s in total\n",
      "INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 57.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 58.53 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 57.21 s in total\n",
      "INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 57.20 s in total\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 58.71 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 58.36 s in total\n",
      "INFO 07-10 18:30:24 [monitor.py:34] torch.compile takes 58.48 s in total\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 110.33 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.96 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:30:27 [gpu_worker.py:227] Available KV cache memory: 109.86 GiB\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,603,120 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.97x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,600,048 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 87.89x\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:715] GPU KV cache size: 3,615,408 tokens\n",
      "INFO 07-10 18:30:28 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 88.27x\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:31:03 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:31:03 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:31:03 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m INFO 07-10 18:31:09 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m INFO 07-10 18:31:09 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:31:09 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:31:09 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:31:11 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=55327)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=55305)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=55317)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=55292)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=55278)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=55261)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=55258)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=55272)\u001b[0;0m INFO 07-10 18:31:11 [gpu_model_runner.py:2048] Graph capturing finished in 44 secs, took 1.13 GiB\n",
      "INFO 07-10 18:31:11 [core.py:171] init engine (profile, create kv cache, warmup model) took 134.02 seconds\n",
      "INFO 07-10 18:31:12 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 225003\n"
     ]
    }
   ],
   "source": [
    "from art.vllm import get_llm\n",
    "import torch\n",
    "import vllm\n",
    "\n",
    "llm = await get_llm(\n",
    "    vllm.AsyncEngineArgs(\n",
    "        model=\"willcb/Qwen3-32B\",\n",
    "        # pipeline_parallel_size=1,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        # data_parallel_size=1,\n",
    "        # enforce_eager=True,\n",
    "        generation_config=\"vllm\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.vllm import openai_server_task\n",
    "\n",
    "sleep_task = await openai_server_task(\n",
    "    engine=llm,\n",
    "    config=art.dev.OpenAIServerConfig(\n",
    "        log_file=\"./vllm.log\",\n",
    "        server_args=art.dev.ServerArgs(\n",
    "            api_key=\"default\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            enable_auto_tool_choice=True,\n",
    "            tool_call_parser=\"hermes\",\n",
    "        ),\n",
    "        engine_args=art.dev.EngineArgs(\n",
    "            model=llm.vllm_config.model_config.model,\n",
    "            disable_log_requests=True,\n",
    "            generation_config=\"vllm\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sleep_task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msleep_task\u001b[49m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sleep_task' is not defined"
     ]
    }
   ],
   "source": [
    "sleep_task.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.vllm import run_on_workers\n",
    "import asyncio\n",
    "import os\n",
    "import signal\n",
    "\n",
    "pids = await run_on_workers(llm, lambda: os.getpid())\n",
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.vllm import get_worker\n",
    "\n",
    "\n",
    "def sleep() -> None:\n",
    "    worker = get_worker()\n",
    "    worker.sleep()\n",
    "    os.kill(os.getpid(), signal.SIGSTOP)\n",
    "    worker.wake_up()\n",
    "\n",
    "\n",
    "sleep_task = asyncio.create_task(run_on_workers(llm, sleep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in pids:\n",
    "    os.kill(pid, signal.SIGCONT)\n",
    "await sleep_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank5]:[W710 18:26:52.260806609 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank5]:[W710 18:26:52.256495309 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=255, addr=[fdff:ffff::3daf:5f6c:207d:0]:22879, remote=[fdff:ffff::]:3661): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d20584d35e8 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7d202d5abafe in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7d202d5ade40 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7d202d5ae74a in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7d202d5a81a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7d1fea7a19a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7d206c622bf4 in /home/sky/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x7d206d1db609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7d206cfa6353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank5]:[W710 18:26:52.260891862 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 5] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank6]:[W710 18:26:52.261004155 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank6]:[W710 18:26:52.256495247 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=259, addr=[fdff:ffff::3daf:5f6c:207d:0]:22879, remote=[fdff:ffff::]:3669): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d20584d35e8 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7d202d5abafe in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7d202d5ade40 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7d202d5ae74a in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7d202d5a81a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7d1fea7a19a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7d206c622bf4 in /home/sky/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x7d206d1db609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7d206cfa6353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank6]:[W710 18:26:52.261071993 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 6] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank7]:[W710 18:26:52.261724878 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank7]:[W710 18:26:52.256910756 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=263, addr=[fdff:ffff::3daf:5f6c:207d:0]:22879, remote=[fdff:ffff::]:3658): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d20584d35e8 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7d202d5abafe in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7d202d5ade40 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7d202d5ae74a in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7d202d5a81a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7d1fea7a19a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7d206c622bf4 in /home/sky/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x7d206d1db609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7d206cfa6353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank7]:[W710 18:26:52.261763071 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 7] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank4]:[W710 18:26:52.386814483 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank4]:[W710 18:26:52.383335839 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=251, addr=[fdff:ffff::3daf:5f6c:207d:0]:22879, remote=[fdff:ffff::]:3661): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d20584d35e8 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7d202d5abafe in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7d202d5ade40 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7d202d5ae74a in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7d202d5a81a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7d1fea7a19a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7d206c622bf4 in /home/sky/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x7d206d1db609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7d206cfa6353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank1]:[W710 18:26:52.386860565 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank1]:[W710 18:26:52.383376864 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=241, addr=[fdff:ffff::3daf:5f6c:207d:0]:22879, remote=[fdff:ffff::]:3661): failed to recv, got 0 bytes\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7d20584d35e8 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5ba8afe (0x7d202d5abafe in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5baae40 (0x7d202d5ade40 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5bab74a (0x7d202d5ae74a in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7d202d5a81a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7d1fea7a19a9 in /home/sky/sky_workdir/.venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdbbf4 (0x7d206c622bf4 in /home/sky/miniconda3/bin/../lib/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x8609 (0x7d206d1db609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7d206cfa6353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank4]:[W710 18:26:52.386877927 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 4] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n",
      "[rank1]:[W710 18:26:52.386912268 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes\n"
     ]
    }
   ],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
