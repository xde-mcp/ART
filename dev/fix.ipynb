{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/gist/bradhilton/a4cfc8d61f52b0c789524afed83d4f89/fix-openpipe-rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "UPDczC8gad44",
                "outputId": "e9c754a6-78e6-4a6a-c8a5-2859563e47a5"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
                        "\u001b[2K\u001b[37m‚†ß\u001b[0m \u001b[2mtriton==3.3.0                                                                 \u001b[0m"
                    ]
                }
            ],
            "source": [
                "!uv pip install -q openpipe-art==0.3.11.post2 langchain-core tenacity \"gql<4\" --prerelease allow --no-cache-dir\n",
                "!uv pip install transformers==4.51.3 trl==0.15.2 vllm==0.9.1 peft==0.15.2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "lTifFi_hbRcG"
            },
            "outputs": [],
            "source": [
                "!uv pip install reasoning-gym==0.1.23"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0lKeQwSuajX0"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "WANDB_API_KEY = \"REDACTED\"  # set your key\n",
                "OPENAI_API_KEY = \"REDACTED\"  # set your OpenAI key\n",
                "OPENPIPE_API_KEY = \"REDACTED\"\n",
                "\n",
                "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
                "\n",
                "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
                "\n",
                "os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "wHDJmbejap96",
                "outputId": "569ceff2-7654-4a94-d83e-50ae61ebdb21"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 07-30 23:13:03 [__init__.py:244] Automatically detected platform cuda.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
                        "  if event.key is 'enter':\n",
                        "\n",
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "  from pkg_resources import resource_stream, resource_exists\n",
                        "\n",
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/cellpylib/ca_functions.py:194: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
                        "  if memoize is \"recursive\":\n",
                        "\n",
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/cellpylib/ca_functions.py:246: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
                        "  if memoize is \"recursive\":\n",
                        "\n",
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/cellpylib/ca_functions2d.py:431: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
                        "  if memoize is \"recursive\":\n",
                        "\n",
                        "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/cellpylib/ca_functions2d.py:501: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
                        "  if memoize is \"recursive\":\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"### Imports & Helpers\n",
                "\"\"\"\n",
                "import re, math, random, time, os, requests\n",
                "from typing import TypedDict\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "import openai\n",
                "from openai import AsyncOpenAI\n",
                "import art\n",
                "from art.local import LocalBackend\n",
                "from pydantic import BaseModel\n",
                "import weave\n",
                "\n",
                "# -- Reasoning Gym\n",
                "import reasoning_gym as rg\n",
                "from reasoning_gym.composite import DatasetSpec\n",
                "from reasoning_gym import get_score_answer_fn\n",
                "\n",
                "load_dotenv()\n",
                "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "random.seed(42)  # reproducibility\n",
                "\n",
                "# -------- Answer extraction --------\n",
                "ANSWER_TAG_RE = re.compile(r\"<answer>(.*?)</answer>\", re.S | re.I)\n",
                "\n",
                "def extract_answer_text(text: str) -> str | None:\n",
                "    \"\"\"Return raw contents inside <answer>...</answer>, trimmed.\n",
                "    If no tags, return None (we'll penalize).\"\"\"\n",
                "    m = ANSWER_TAG_RE.search(text or \"\")\n",
                "    if not m:\n",
                "        return None\n",
                "    return m.group(1).strip()\n",
                "\n",
                "# -------- RG task spec --------\n",
                "RG_SPECS = [\n",
                "    # Arithmetic\n",
                "    DatasetSpec(\"basic_arithmetic\",          weight=3, config={}),\n",
                "    DatasetSpec(\"decimal_arithmetic\",        weight=2, config={}),\n",
                "    DatasetSpec(\"fraction_simplification\",   weight=2, config={}),\n",
                "    DatasetSpec(\"power_function\",            weight=1, config={}),\n",
                "    DatasetSpec(\"time_intervals\",            weight=1, config={}),\n",
                "    DatasetSpec(\"calendar_arithmetic\",       weight=1, config={}),\n",
                "    DatasetSpec(\"leg_counting\",              weight=1, config={}),\n",
                "\n",
                "    # Algorithms\n",
                "    DatasetSpec(\"prime_factorization\",       weight=2, config={}),  # has min_value/max_value\n",
                "    DatasetSpec(\"gcd\",                       weight=1, config={}),\n",
                "    DatasetSpec(\"lcm\",                       weight=1, config={}),\n",
                "    DatasetSpec(\"count_primes\",              weight=1, config={}),\n",
                "    DatasetSpec(\"count_bits\",                weight=1, config={}),\n",
                "    DatasetSpec(\"chain_sum\",                 weight=1, config={}),\n",
                "    DatasetSpec(\"decimal_chain_sum\",         weight=1, config={}),\n",
                "    DatasetSpec(\"base_conversion\",           weight=1, config={}),\n",
                "    DatasetSpec(\"number_sorting\",            weight=1, config={}),\n",
                "\n",
                "    # Geometry\n",
                "    DatasetSpec(\"rectangle_count\",           weight=1, config={}),\n",
                "    DatasetSpec(\"simple_geometry\",           weight=1, config={}),\n",
                "\n",
                "    # Induction\n",
                "    DatasetSpec(\"number_sequence\",           weight=1, config={}),\n",
                "]\n",
                "\n",
                "def make_rg_entry(step: int, seed: int | None = None):\n",
                "    \"\"\"Generate a single RG sample (composite, weighted).\"\"\"\n",
                "    if seed is None:\n",
                "        seed = random.randint(0, 2**31 - 1)\n",
                "\n",
                "    # You can implement a curriculum by varying configs based on `step`.\n",
                "    # For now we rely on dataset defaults (already balanced easy‚Üímedium).\n",
                "    data = rg.create_dataset(\n",
                "        \"composite\",\n",
                "        size=1,\n",
                "        seed=seed,\n",
                "        datasets=RG_SPECS,\n",
                "    )\n",
                "    entry = data[0]  # dict with keys: question, answer, metadata\n",
                "    return entry\n",
                "\n",
                "def score_rg_answer(entry: dict, answer_text: str) -> float:\n",
                "    \"\"\"Returns 1.0 if correct, else 0.0. Uses RG‚Äôs official verifier.\"\"\"\n",
                "    dataset_name = entry[\"metadata\"][\"source_dataset\"]\n",
                "    rg_score = get_score_answer_fn(dataset_name)\n",
                "    return float(rg_score(answer_text, entry))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "2Znl5zySar5R"
            },
            "outputs": [],
            "source": [
                "\"\"\"### Declare the Trainable Model\n",
                "\"\"\"\n",
                "\n",
                "model = art.TrainableModel(\n",
                "    name='testname',\n",
                "    project    = \"autotransmission-single-turn\",\n",
                "    base_model = \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
                ")\n",
                "model._internal_config = art.dev.InternalModelConfig(\n",
                "    init_args   = art.dev.InitArgs(max_seq_length=4096),\n",
                "    engine_args = art.dev.EngineArgs(enforce_eager=True, gpu_memory_utilization=0.7),\n",
                ")\n",
                "\n",
                "backend = LocalBackend(in_process=True, path=\"./.art\")\n",
                "await model.register(backend)\n",
                "\n",
                "weave.init(model.project)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "UDPEt0z2a_QM"
            },
            "outputs": [],
            "source": [
                "\"\"\"### Rollout Scenario\n",
                "\"\"\"\n",
                "class ScenarioAIME(BaseModel):\n",
                "    step: int\n",
                "\n",
                "\"\"\"### Rollout Function (now uses Reasoning Gym)\n",
                "\n",
                "We train Qwen to *rewrite* the user problem into a great prompt for GPT‚Äë4.1‚Äënano.\n",
                "We then score GPT‚Äë4.1‚Äënano‚Äôs final <answer> against RG‚Äôs verifier.\n",
                "Reward = 2 if correct, 0 if incorrect, ‚àí1 if no <answer>.\n",
                "\"\"\"\n",
                "\n",
                "@weave.op\n",
                "@art.retry(exceptions=(requests.ReadTimeout,))\n",
                "async def rollout(model: art.Model, _unused, scenario: ScenarioAIME) -> art.Trajectory:\n",
                "    # --- sample a fresh RG entry ------------------------------------------------\n",
                "    entry = make_rg_entry(step=scenario.step)\n",
                "    problem_text = entry[\"question\"]                 # plain text question\n",
                "    gold_answer = entry[\"answer\"]                    # ground-truth string (not used directly)\n",
                "    source_ds    = entry[\"metadata\"][\"source_dataset\"]\n",
                "\n",
                "    # --- init trajectory --------------------------------------------------------\n",
                "    # sys_prompt = (\n",
                "    #     \"You are a prompt improver. Given a user problem, rewrite an instruction \"\n",
                "    #     \"for GPT‚Äë4.1‚Äënano to SOLVE it, including precise format requirements. \"\n",
                "    #     \"Do NOT solve it yourself. If the task expects a list or grid, instruct the exact \"\n",
                "    #     \"required spacing and ordering for the final answer. Ensure that GPT-4.1-nano is instructed to put its final answer after thinking inside <answer> tags, otherwise, it will not be counted. You may instruct it to do anything you want it to to get to the <answer> before committing to it. Ensure the ONLY thing inside the <answer> tags is the actual answer, no other text or explanation. For example: <answer>43-24=19</answer> includes explanation, so it would not be counted. But <answer>19</answer> will be counted.\"\n",
                "    # )\n",
                "\n",
                "    sys_prompt = \"\"\"<task_description>\n",
                "You will be given a question (<original_question>). Your job is to rewrite this question in a way that will help another model (gpt-4.1-nano) answer it correctly. The prompt you rewrite will be the *only* thing gpt-4.1-nano generates against, so make it clear, faithful to the <original_question>, and easier to answer properly than just the raw <original_question>. Think: \"what reframing would make it easier to the model to arrive at the correct answer?\"\n",
                "\n",
                "You should make the question clearer and easier to understand and answer while keeping the same meaning.\n",
                "</task_description>\n",
                "\n",
                "<IMPORTANT>\n",
                "Ensure that GPT-4.1-nano is instructed to put its final answer after thinking inside <answer> tags, otherwise, it will not be counted. You may instruct it to do anything you want it to to get to the <answer> before committing to it. Ensure the ONLY thing inside the <answer> tags is the actual answer, no other text or explanation. For example: <answer>43-24=19</answer> includes explanation, so it would not be counted. But <answer>19</answer> will be counted.\n",
                "</IMPORTANT>\n",
                "\n",
                "<examples>\n",
                "**Example 1 ‚Äî Math word problem (structured variables)**\n",
                "\n",
                "* **Original:** ‚ÄúA car drives 60 mph for 2 hours and 30 mph for 1 hour. What is the average speed for the whole trip?‚Äù\n",
                "* **Rewritten:**\n",
                "  ‚ÄúYou are given two travel segments.\n",
                "\n",
                "  * Segment A: speed = 60 mph, time = 2 h\n",
                "  * Segment B: speed = 30 mph, time = 1 h\n",
                "    Let total distance `D = 60*2 + 30*1`. Let total time `T = 2 + 1`. Compute average speed `S_avg = D / T` in mph.\n",
                "    Show your calculation steps if needed, then give only the numeric average speed (mph not included) as the final result inside `<answer>` tags.\n",
                "    **Final answer format:** `<answer>{number}</answer>`.‚Äù\n",
                "\n",
                "**Example 2 ‚Äî Unit conversion (explicit constants + IO spec)**\n",
                "\n",
                "* **Original:** ‚ÄúHow many grams are in 2.5 pounds?‚Äù\n",
                "* **Rewritten:**\n",
                "  ‚ÄúConvert mass from pounds to grams. Use the exact factor `1 lb = 453.59237 g`.\n",
                "  Compute `grams = 2.5 * 453.59237`. Round to the nearest whole number.\n",
                "  Output only the rounded integer inside `<answer>` tags (no units).\n",
                "  **Final answer format:** `<answer>{integer}</answer>`.‚Äù\n",
                "\n",
                "**Example 3 ‚Äî Logic puzzle (edge cases + minimality)**\n",
                "\n",
                "* **Original:** ‚ÄúThere are three boxes labeled ‚Äòapples‚Äô, ‚Äòoranges‚Äô, and ‚Äòmixed‚Äô, but all labels are wrong. What is the smallest number of fruit you must draw to label the boxes correctly?‚Äù\n",
                "* **Rewritten:**\n",
                "  ‚ÄúYou have 3 mislabeled boxes: A, O, M. Each contains {only apples}, {only oranges}, or {a mix}, and each current label is incorrect. Determine the **minimum** number of single‚Äëfruit draws (without looking) needed to deduce the true contents of all three boxes, assuming you can choose which box to draw from and observe the fruit type drawn. To do this, first think for at least 25 paragraphs inside <thinking> tags, and then write your answer inside <answer> tags.\n",
                "  Provide only the minimal required number inside `<answer>` tags.\n",
                "  **Final answer format:** `<answer>{integer}</answer>`.‚Äù\n",
                "\n",
                "**Example 4 ‚Äî Programming/string rules (clarify requirements)**\n",
                "\n",
                "* **Original:** ‚ÄúCompress ‚Äòaaabbcaaa‚Äô with run-length encoding.‚Äù\n",
                "* **Rewritten:**\n",
                "  ‚ÄúApply simple run‚Äëlength encoding (RLE) to the ASCII string `aaabbcaaa` with these rules:\n",
                "\n",
                "  * Consecutive runs are encoded as `<char><count>`.\n",
                "  * If a run length is 1, write only the character (no ‚Äò1‚Äô).\n",
                "  * Example: `AAB` ‚Üí `A2B`.\n",
                "    Compute the RLE of `aaabbcaaa` and return only the encoded string inside `<answer>` tags.\n",
                "    **FINAL ANSWER SHOULD BE IN THIS FORMAT:** `<answer>{string}</answer>`.‚Äù\n",
                "\n",
                "**Example 5 ‚Äî Geography (disambiguation + output constraints)**\n",
                "\n",
                "* **Original:** ‚ÄúWhich African country borders both the Atlantic Ocean and the Mediterranean Sea?‚Äù\n",
                "* **Rewritten:**\n",
                "  ‚ÄúFind the **single** African country that has coastlines on **both** the Atlantic Ocean and the Mediterranean Sea. To do this, first, list all African countries. Then, for each, one by one, list the coastlines (with specifics) that each country has. From there, select the one that has coastlines on **both** the Atlantic Ocean and the Mediterranean Sea. Once you are sure of your answer, return only the country name, with standard English spelling, inside `<answer>` tags.\n",
                "  **Place your final answer in this format at the end of your response:** `<answer>{country}</answer>`.‚Äù\n",
                "</examples>\n",
                "\n",
                "Remember, your job is to rewrite the <original_question> to make it more likely for gpt-4.1-nano to get the correct answer. Do this incredibly well.\"\"\"\n",
                "\n",
                "    trajectory = art.Trajectory(\n",
                "        messages_and_choices=[\n",
                "            {\"role\": \"system\", \"content\": sys_prompt},\n",
                "            {\"role\": \"user\",   \"content\": f\"<original_question>{problem_text}</original_question>\\n\\nRewrite the <original_question> to make it more likely for gpt-4.1-nano to get the correct answer. DO NOT solve it yourself. Just rewrite the <original_question>.\"},\n",
                "        ],\n",
                "        metadata={\n",
                "            \"problem_id\"   : f\"{source_ds}:{entry['metadata'].get('source_index', 'na')}\",\n",
                "            \"step\"         : scenario.step,\n",
                "            \"rg_dataset\"   : source_ds,\n",
                "        },\n",
                "        reward=0,\n",
                "    )\n",
                "\n",
                "    # --- Qwen produces the improved prompt -------------------------------------\n",
                "    client = model.openai_client()  # Qwen inference\n",
                "    try:\n",
                "        chat = await client.chat.completions.create(\n",
                "            messages      = trajectory.messages(),\n",
                "            model         = model.name,\n",
                "            max_tokens    = 2000,\n",
                "            stream        = False,\n",
                "            temperature   = 1.0,\n",
                "            stop          = [],\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(\"Qwen completion error:\", e)\n",
                "        trajectory.reward = -1\n",
                "        return trajectory\n",
                "\n",
                "    choice  = chat.choices[0]\n",
                "    qwen_prompt = (choice.message.content or \"\").strip()\n",
                "    trajectory.messages_and_choices.append(choice)\n",
                "\n",
                "    # --- GPT‚Äë4.1‚Äënano solves with strict formatting ----------------------------\n",
                "    # Official model name documented by OpenAI. :contentReference[oaicite:8]{index=8}\n",
                "    openai_client = AsyncOpenAI()\n",
                "    tool_resp = await openai_client.chat.completions.create(\n",
                "        model    = \"gpt-4.1-nano\",\n",
                "        messages = [\n",
                "            # {\n",
                "            #     \"role\": \"system\",\n",
                "            #     \"content\": (\n",
                "            #         \"Place your final answer inside <answer> tags, otherwise, it will not be counted. You may do anything you want to get to the <answer> before committing to it. Ensure the ONLY thing inside the <answer> tags is the actual answer, no other text or explanation. For example: <answer>43-24=19</answer> includes explanation, so it would not be counted. But <answer>19</answer> will be counted.\"\n",
                "            #     ),\n",
                "            # },\n",
                "            {\"role\": \"user\", \"content\": qwen_prompt},\n",
                "        ],\n",
                "        temperature = 0.0,\n",
                "    )\n",
                "    tool_content = (tool_resp.choices[0].message.content or \"\").strip()\n",
                "\n",
                "    # --- extract final answer text & score -------------------------------------\n",
                "    answer_text = extract_answer_text(tool_content)\n",
                "    if answer_text is None:\n",
                "        trajectory.reward = -1\n",
                "    else:\n",
                "        score = score_rg_answer(entry, answer_text)   # 1.0 or 0.0\n",
                "        trajectory.reward = 2 if score >= 1.0 else 0\n",
                "        print(\n",
                "            f\"Step {scenario.step}  |  DS={source_ds:<20}  \"\n",
                "            f\"{'Correct' if score>=1.0 else 'Wrong'}  \"\n",
                "            f\"ANS='{answer_text}'\"\n",
                "            f\"CORRECT={entry['answer']}\"\n",
                "        )\n",
                "\n",
                "    return trajectory\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "_qEg6SfdcF0X"
            },
            "outputs": [],
            "source": [
                "\"\"\"### Training Loop (RG procedural data)\n",
                "\"\"\"\n",
                "# ---- pick how many problems per step and rollouts per problem\n",
                "# P = 4                      # problems per step\n",
                "# R = 8                      # rollouts per problem\n",
                "# NUM_EPOCHS = 2\n",
                "# NUM_STEPS_PER_EPOCH = 64\n",
                "\n",
                "P = 1                      # problems per step\n",
                "R = 2                      # rollouts per problem\n",
                "NUM_EPOCHS = 2\n",
                "NUM_STEPS_PER_EPOCH = 128\n",
                "\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    print(f\"=== Epoch {epoch+1}/{NUM_EPOCHS} ===\")\n",
                "    for step in range(NUM_STEPS_PER_EPOCH):\n",
                "        global_step = epoch * NUM_STEPS_PER_EPOCH + step\n",
                "\n",
                "        # Sample P entries (you can bias by weights yourself if desired)\n",
                "        entries = [make_rg_entry(step=global_step) for _ in range(P)]\n",
                "\n",
                "        train_groups_iter = []\n",
                "        for entry in entries:\n",
                "            tg = art.TrajectoryGroup(\n",
                "                rollout(model, entry, ScenarioAIME(step=global_step))\n",
                "                for _ in range(R)\n",
                "            )\n",
                "            train_groups_iter.append(tg)\n",
                "\n",
                "        train_groups = await art.gather_trajectory_groups(\n",
                "            tuple(train_groups_iter),\n",
                "            pbar_desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}  Step {step+1}/{NUM_STEPS_PER_EPOCH}\",\n",
                "            max_exceptions=18,\n",
                "        )\n",
                "\n",
                "        await model.train(\n",
                "            train_groups,\n",
                "            config  = art.TrainConfig(learning_rate=0.0000025),\n",
                "            _config = {\"logprob_calculation_chunk_size\": 8},\n",
                "        )\n",
                "\n",
                "        await model.delete_checkpoints()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 525
                },
                "id": "Qub2XVRvcHxI",
                "outputId": "88bf4016-8ff4-43dd-b992-2be6f4e2623d"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/otherside/autotransmission-single-turn/r/call/01985d76-2057-7362-b195-3ab68e897106\n"
                    ]
                },
                {
                    "ename": "APIConnectionError",
                    "evalue": "Connection error.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/auto.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         return await self._backend.connect_tcp(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    112\u001b[0m         }\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_exceptions.py\u001b[0m in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mto_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1526\u001b[0;31m                 response = await self._client.send(\n\u001b[0m\u001b[1;32m   1527\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mmapped_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-2993012342.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Qwen inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m chat = await client.chat.completions.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmessages\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/art/openai.py\u001b[0m in \u001b[0;36mcreate_patched\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatCompletion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mreport_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pyright: ignore[reportRedeclaration]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     res, _ = await _call_async_func(\n\u001b[0m\u001b[1;32m   1270\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     )\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_async_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weave/integrations/openai/openai_sdk.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhostname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"api.openai.com\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2452\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   2453\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   2455\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m             body=await async_maybe_transform(\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         )\n\u001b[0;32m-> 1791\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m     async def patch(\n",
                        "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Raising connection error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAPIConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             log.debug(\n",
                        "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
                    ]
                }
            ],
            "source": [
                "client = model.openai_client()  # Qwen inference\n",
                "\n",
                "chat = await client.chat.completions.create(\n",
                "    messages      = [{\"role\": \"user\", \"content\": \"hi\"}],\n",
                "    model         = model.name,\n",
                "    max_tokens    = 20,\n",
                "    stream        = False,\n",
                "    temperature   = 1.0,\n",
                "    stop          = [],\n",
                ")\n",
                "\n",
                "\n",
                "choice  = chat.choices[0]\n",
                "choice"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "i7zrzg7ZdaEO"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "include_colab_link": true,
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
