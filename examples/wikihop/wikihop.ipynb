{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train this agent, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to navigate Wikipedia. It will demonstrate how to set up a multi-turn agent that learns to hop between Wikipedia pages by selecting the best links to reach target pages.\n",
        "\n",
        "Completions will be logged to OpenPipe, and metrics will be logged to Weights & Biases.\n",
        "\n",
        "You will learn how to construct an [agentic environment](#Environment), how to define a [rollout](#Rollout), and how to run a [training loop](#Loop).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.10.13 environment at: /root/sky_workdir/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install \"numpy<2.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WARNING:\n",
        "\n",
        "If you are running in Google Colab and installing numpy does not say \"Requirement already satisfied: numpy<2.0.0\" then click \"Runtime\" and \"Restart Session.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy version is 1.*.*, you're good to go!\n"
          ]
        }
      ],
      "source": [
        "# make sure we're using numpy 1.*.*\n",
        "import numpy as np\n",
        "\n",
        "if (np.__version__).startswith(\"1.\"):\n",
        "    print(\"Numpy version is 1.*.*, you're good to go!\")\n",
        "else:\n",
        "    raise ValueError(\"Please restart your runtime using the above instructions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Variables\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that can automatically logs metrics to Weights & Biases. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n",
        "\n",
        "You can also optionally initiate an OpenPipe client to report completions to a [dashboard](https://app.openpipe.ai) to get a feel for what the completions your model is generating look like, and how they change over time. Logging to OpenPipe is free, but is not required for training!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Optional\n",
        "WANDB_API_KEY = \"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "# Optional\n",
        "OPENPIPE_API_KEY = \"\"\n",
        "if OPENPIPE_API_KEY:\n",
        "    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv pip install openpipe-art==0.3.11 openpipe accelerate==1.7.0 requests beautifulsoup4 --prerelease allow --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agentic Environment\n",
        "\n",
        "<a name=\"Environment\"></a>\n",
        "\n",
        "ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment in which the agent navigates Wikipedia by selecting links to reach target pages.\n",
        "\n",
        "The agent starts at the Philosophy Wikipedia page and must navigate to various target pages by selecting the best links from the first paragraph of each page. The environment functions handle Wikipedia scraping, link selection, and target matching.\n",
        "\n",
        "Feel free to read as much or as little of this section's code as you'd like. The important thing to understand is that we're defining the rules of this agent's environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from typing import List\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def read_first_paragraphs_with_links(url: str, num_paragraphs: int = 3) -> str:\n",
        "    \"\"\"Given a link to a wikipedia page, read the first paragraphs with links.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Find the main content div\n",
        "        content = soup.find('div', {'id': 'mw-content-text'})\n",
        "        if not content:\n",
        "            return \"Could not find main content\"\n",
        "\n",
        "        num_good_paragraphs = 0\n",
        "        parsed_paragraphs = []\n",
        "        # also count ordered and unordered lists\n",
        "        paragraphs = content.find_all(['p', 'ol', 'ul'])\n",
        "        paragraph_index = 0\n",
        "\n",
        "        while num_good_paragraphs < num_paragraphs and paragraph_index < len(paragraphs):\n",
        "            p = paragraphs[paragraph_index]\n",
        "            text = p.get_text().strip()\n",
        "            # Get the paragraph with links preserved\n",
        "            paragraph_html = str(p)\n",
        "            parsed_paragraphs.append(paragraph_html)\n",
        "            if len(text) > 50 and p.find('a'):  # Must have substantial content and a link\n",
        "                num_good_paragraphs += 1\n",
        "            paragraph_index += 1\n",
        "        \n",
        "        if len(parsed_paragraphs) == 0:\n",
        "            raise Exception(\"No substantial first paragraphs found\")\n",
        "        \n",
        "        return \"\\n\\n\".join(parsed_paragraphs)\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error reading page: {str(e)}\"\n",
        "\n",
        "\n",
        "def extract_urls(first_paragraph: str) -> List[str]:\n",
        "    \"\"\"Extract Wikipedia URLs from the first paragraph HTML.\"\"\"\n",
        "    soup = BeautifulSoup(first_paragraph, 'html.parser')\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        text = a_tag.get_text().strip()\n",
        "        if href.startswith('/wiki/') and ':' not in href and text:\n",
        "            full_url = f\"https://en.wikipedia.org{href}\"\n",
        "            links.append(full_url)\n",
        "    return links\n",
        "\n",
        "\n",
        "\n",
        "def check_target_match(current_url: str, target_page: str) -> bool:\n",
        "    \"\"\"Given the current url, determine whether it is a match for the target page.\"\"\"\n",
        "    # Normalize URLs for comparison\n",
        "    current_url = current_url.strip().rstrip('/')\n",
        "    target_page = target_page.strip().rstrip('/')\n",
        "    \n",
        "    # Direct match\n",
        "    if current_url == target_page:\n",
        "        return True\n",
        "    \n",
        "    # Extract the page title from both URLs\n",
        "    def extract_page_title(url):\n",
        "        if '/wiki/' in url:\n",
        "            return url.split('/wiki/')[-1]\n",
        "        return url\n",
        "    \n",
        "    current_title = extract_page_title(current_url)\n",
        "    target_title = extract_page_title(target_page)\n",
        "\n",
        "    \n",
        "    return current_title == target_title\n",
        "\n",
        "\n",
        "# Target URLs for training scenarios\n",
        "TARGET_URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Unsupervised_learning\",\n",
        "    \"https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma\",\n",
        "    \"https://en.wikipedia.org/wiki/Markov_decision_process\",\n",
        "    \"https://en.wikipedia.org/wiki/Autoencoder\",\n",
        "    \"https://en.wikipedia.org/wiki/Gradient_descent\",\n",
        "    \"https://en.wikipedia.org/wiki/Data_compression\",\n",
        "    \"https://en.wikipedia.org/wiki/Barcode\",\n",
        "    \"https://en.wikipedia.org/wiki/Edge_detection\",\n",
        "    \"https://en.wikipedia.org/wiki/Evolutionary_algorithm\",\n",
        "    \"https://en.wikipedia.org/wiki/Fitness_function\",\n",
        "    \"https://en.wikipedia.org/wiki/Planning\",\n",
        "    \"https://en.wikipedia.org/wiki/Forecasting\",\n",
        "    \"https://en.wikipedia.org/wiki/Agentic_AI\",\n",
        "    \"https://en.wikipedia.org/wiki/Outer_space\",\n",
        "    \"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\",\n",
        "    \"https://en.wikipedia.org/wiki/Particle_physics\",\n",
        "    \"https://en.wikipedia.org/wiki/Hadron\",\n",
        "    \"https://en.wikipedia.org/wiki/Ancient_Greek\",\n",
        "    \"https://en.wikipedia.org/wiki/Logic\",\n",
        "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
        "    \"https://en.wikipedia.org/wiki/Action_selection\",\n",
        "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
        "    \"https://en.wikipedia.org/wiki/Golden_Rule\"\n",
        "]\n",
        "\n",
        "STARTING_URL = \"https://en.wikipedia.org/wiki/Reinforcement_learning\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['https://en.wikipedia.org/wiki/Cooking', 'https://en.wikipedia.org/wiki/Arabic_language', 'https://en.wikipedia.org/wiki/Latin_language', 'https://en.wikipedia.org/wiki/Viscous', 'https://en.wikipedia.org/wiki/Solution_(chemistry)', 'https://en.wikipedia.org/wiki/Sugar', 'https://en.wikipedia.org/wiki/Crystal', 'https://en.wikipedia.org/wiki/Molasses', 'https://en.wikipedia.org/wiki/Hydrogen_bond', 'https://en.wikipedia.org/wiki/Hydroxyl', 'https://en.wikipedia.org/wiki/Agave_nectar', 'https://en.wikipedia.org/wiki/Agave', 'https://en.wikipedia.org/wiki/Cane_syrup', 'https://en.wikipedia.org/wiki/Chocolate_syrup', 'https://en.wikipedia.org/wiki/Corn_syrup', 'https://en.wikipedia.org/wiki/Glucose_syrup', 'https://en.wikipedia.org/wiki/Golden_syrup', 'https://en.wikipedia.org/wiki/Sugar', 'https://en.wikipedia.org/wiki/High_fructose_corn_syrup', 'https://en.wikipedia.org/wiki/Maple_syrup', 'https://en.wikipedia.org/wiki/Table_syrup', 'https://en.wikipedia.org/wiki/Mixed_drink']\n"
          ]
        }
      ],
      "source": [
        "print(extract_urls(read_first_paragraphs_with_links(\"https://en.wikipedia.org/wiki/Syrup\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to navigate Wikipedia. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenPipe client initialized\n"
          ]
        }
      ],
      "source": [
        "import art\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from openpipe.client import OpenPipe\n",
        "from art.local import LocalBackend\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "op_client = OpenPipe()\n",
        "print(\"OpenPipe client initialized\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "backend = LocalBackend(path=\"./.art\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Registration\n",
        "\n",
        "Now we'll register our model with the ART backend. This creates the infrastructure needed for training and tracks our model's progress through the training steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/sky_workdir/.venv/lib/python3.10/site-packages/art/__init__.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth  # type: ignore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 07-01 20:06:42 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 07-01 20:06:42 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.5.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
            "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.19 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
            "Unsloth: vLLM's KV Cache can use up to 56.27 GB. Also swap space = 6 GB.\n",
            "INFO 07-01 20:06:59 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 07-01 20:06:59 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n",
            "INFO 07-01 20:07:00 [cuda.py:292] Using Flash Attention backend.\n",
            "INFO 07-01 20:07:01 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 07-01 20:07:01 [model_runner.py:1108] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
            "INFO 07-01 20:07:01 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 07-01 20:07:01 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.65it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.67it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-01 20:07:04 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 07-01 20:07:05 [model_runner.py:1140] Model loading took 6.7357 GiB and 3.758575 seconds\n",
            "INFO 07-01 20:07:09 [worker.py:287] Memory profiling takes 4.04 seconds\n",
            "INFO 07-01 20:07:09 [worker.py:287] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.78) = 62.14GiB\n",
            "INFO 07-01 20:07:09 [worker.py:287] model weights take 6.74GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.72GiB; the rest of the memory reserved for KV Cache is 50.54GiB.\n",
            "INFO 07-01 20:07:09 [executor_base.py:112] # cuda blocks: 59141, # CPU blocks: 7021\n",
            "INFO 07-01 20:07:09 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 28.88x\n",
            "INFO 07-01 20:07:14 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:42<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-01 20:07:56 [model_runner.py:1592] Graph capturing finished in 42 secs, took 1.32 GiB\n",
            "INFO 07-01 20:07:56 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 51.05 seconds\n",
            "Unsloth: Just some info: will skip parsing ['q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'k_norm']\n",
            "Unsloth: Just some info: will skip parsing ['q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm', 'k_norm']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.5.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if TEST_MODE:\n",
        "    model_or = art.Model(\n",
        "        name=\"closed_or\",\n",
        "        project=\"wikihop-navigation\",\n",
        "        inference_base_url=\"https://openrouter.ai/api/v1\",\n",
        "        inference_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "        inference_model_name=\"openai/gpt-4.1\",\n",
        "    )\n",
        "    model = art.Model(\n",
        "        name=\"closed\",\n",
        "        project=\"wikihop-navigation\",\n",
        "        inference_base_url=\"https://api.openai.com/v1\",\n",
        "        inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        inference_model_name=\"gpt-4.1\",\n",
        "    )\n",
        "else:\n",
        "\n",
        "    model = art.TrainableModel(\n",
        "        name=\"006-wikihop\", project=\"wikihop-navigation\", base_model=\"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    )\n",
        "    await model.register(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining a Rollout\n",
        "\n",
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. It generates one or more trajectories, which are lists of messages and choices.\n",
        "\n",
        "In this example, the rollout function starts the agent at the Philosophy Wikipedia page and gives it a target page to reach. The agent navigates by selecting links from the first paragraph of each page until it either reaches the target or makes too many moves.\n",
        "\n",
        "When the navigation is finished, the `reward` for the agent's performance is calculated based on whether it successfully reached the target, how many steps it took, and whether it made any errors.\n",
        "\n",
        "This rollout function will be called many times in parallel during each step of the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "import openai\n",
        "import time\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class WikihopScenario(BaseModel):\n",
        "    step: int\n",
        "    target_url: str\n",
        "\n",
        "max_hops = 20\n",
        "\n",
        "\n",
        "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
        "async def rollout(\n",
        "    model: art.Model, scenario: WikihopScenario\n",
        ") -> art.Trajectory:\n",
        "    current_url = STARTING_URL\n",
        "    target_url = scenario.target_url\n",
        "    \n",
        "    trajectory = art.Trajectory(\n",
        "        messages_and_choices=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a Wikipedia navigator. Your goal is to reach the target page: {target_url}\\n\\nYou will be shown the first few paragraphs of each Wikipedia page. Select the link from these paragraphs that is most likely to get you closer to your target. If you see a direct link to your target, choose it immediately.\\n\\nRespond with ONLY the full URL in this exact format: <url>https://en.wikipedia.org/wiki/YourChoice</url>. You must always choose a link from the list of available links.\",\n",
        "            }\n",
        "        ],\n",
        "        reward=0,\n",
        "    )\n",
        "\n",
        "    hop_number = 0\n",
        "    \n",
        "    while hop_number < max_hops:\n",
        "        # Check if we've reached the target\n",
        "        if check_target_match(current_url, target_url):\n",
        "            print(\"reached target\")\n",
        "            trajectory.reward = max_hops - hop_number  # Negative of total hops used\n",
        "            trajectory.metrics[\"success\"] = 1\n",
        "            trajectory.metrics[\"hops\"] = hop_number\n",
        "            break\n",
        "            \n",
        "        # Read the first paragraph of the current page\n",
        "        try:\n",
        "            first_paragraph = read_first_paragraphs_with_links(current_url)\n",
        "            if first_paragraph.startswith(\"Error\"):\n",
        "                trajectory.reward = hop_number - max_hops\n",
        "                trajectory.metrics[\"success\"] = 0\n",
        "                trajectory.metrics[\"hops\"] = hop_number\n",
        "                trajectory.metadata[\"error\"] = \"page_read_error\"\n",
        "                break\n",
        "        except Exception as e:\n",
        "            trajectory.reward = hop_number - max_hops\n",
        "            trajectory.metrics[\"success\"] = 0\n",
        "            trajectory.metrics[\"hops\"] = hop_number\n",
        "            trajectory.metadata[\"error\"] = \"page_read_exception\"\n",
        "            break\n",
        "\n",
        "        # Extract links from the first paragraph\n",
        "        links = extract_urls(first_paragraph)\n",
        "        \n",
        "        if not links:\n",
        "            trajectory.reward = hop_number - max_hops\n",
        "            trajectory.metrics[\"success\"] = 0\n",
        "            trajectory.metrics[\"hops\"] = hop_number\n",
        "            trajectory.metadata[\"error\"] = \"no_valid_links\"\n",
        "            break\n",
        "\n",
        "        links_text = \"\\n\".join(links)\n",
        "\n",
        "        # Add the current page content to the trajectory\n",
        "        page_title = current_url.split('/wiki/')[-1].replace('_', ' ')\n",
        "        trajectory.messages_and_choices.append({\n",
        "            \"role\": \"user\", \n",
        "            \"content\": f\"Current page: {page_title}\\nHop {hop_number + 1}/{max_hops}\\n\\nChoose from one of the following available links:\\n{links_text}\"\n",
        "        })\n",
        "\n",
        "        requested_at = int(time.time() * 1000)\n",
        "\n",
        "        try:\n",
        "            \n",
        "            \n",
        "            # Get the model's choice of next URL\n",
        "            client = model.openai_client()\n",
        "\n",
        "            chat_completion = None\n",
        "            chat_completion = await client.chat.completions.create(\n",
        "                model=model.get_inference_name(),\n",
        "                messages=trajectory.messages(),\n",
        "                max_completion_tokens=2000,\n",
        "            )\n",
        "\n",
        "            \n",
        "            response = chat_completion.choices[0].message.content\n",
        "            if not response:\n",
        "                trajectory.reward = hop_number - max_hops\n",
        "                trajectory.metrics[\"success\"] = 0\n",
        "                trajectory.metrics[\"hops\"] = hop_number\n",
        "                trajectory.metadata[\"error\"] = \"empty_model_response\"\n",
        "                break\n",
        "            \n",
        "            # Parse the XML response to get the selected URL\n",
        "            try:\n",
        "                root = ET.fromstring(response)\n",
        "                next_url = root.text\n",
        "                if not next_url:\n",
        "                    trajectory.reward = hop_number - max_hops\n",
        "                    trajectory.metrics[\"success\"] = 0\n",
        "                    trajectory.metrics[\"hops\"] = hop_number\n",
        "                    trajectory.metadata[\"error\"] = \"empty_url_in_response\"\n",
        "                    break\n",
        "                next_url = next_url.strip()\n",
        "            except ET.ParseError:\n",
        "                # Try to extract URL with regex as fallback\n",
        "                url_pattern = r'https://en\\.wikipedia\\.org/wiki/[^\\s<>]+'\n",
        "                matches = re.findall(url_pattern, response)\n",
        "                if matches:\n",
        "                    next_url = matches[0]\n",
        "                else:\n",
        "                    trajectory.reward = hop_number - max_hops\n",
        "                    trajectory.metrics[\"success\"] = 0\n",
        "                    trajectory.metrics[\"hops\"] = hop_number\n",
        "                    trajectory.metadata[\"error\"] = \"could_not_parse_url\"\n",
        "                    break\n",
        "\n",
        "            print(target_url.split('/wiki/')[-1], next_url)\n",
        "            \n",
        "            # Validate that the selected URL is actually in the first paragraph\n",
        "            if next_url not in links:\n",
        "                trajectory.reward = hop_number - max_hops\n",
        "                trajectory.metrics[\"success\"] = 0\n",
        "                trajectory.metrics[\"hops\"] = hop_number\n",
        "                trajectory.metadata[\"error\"] = \"selected_url_not_in_text\"\n",
        "                break\n",
        "            \n",
        "        except openai.LengthFinishReasonError as e:\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print(f\"caught exception during URL selection: {e}\")\n",
        "            trajectory.reward = hop_number - max_hops\n",
        "            trajectory.metrics[\"success\"] = 0\n",
        "            trajectory.metrics[\"hops\"] = hop_number\n",
        "            trajectory.metadata[\"error\"] = \"url_selection_error\"\n",
        "            break\n",
        "\n",
        "        # Record the choice\n",
        "        trajectory.messages_and_choices.append(chat_completion.choices[0])\n",
        "\n",
        "        # Move to the next page\n",
        "        current_url = next_url\n",
        "        hop_number += 1\n",
        "\n",
        "    # If we ran out of hops without reaching the target\n",
        "    if hop_number >= max_hops and not check_target_match(current_url, target_url):\n",
        "        trajectory.reward = hop_number - max_hops  # Error penalty for not reaching target, but reward good initial hops\n",
        "        trajectory.metrics[\"success\"] = 0\n",
        "        trajectory.metrics[\"hops\"] = hop_number\n",
        "        trajectory.metadata[\"error\"] = \"max_hops_exceeded\"\n",
        "    \n",
        "    # Final metrics\n",
        "    if \"success\" not in trajectory.metrics:\n",
        "        trajectory.metrics[\"success\"] = 1 if check_target_match(current_url, target_url) else 0\n",
        "    if \"hops\" not in trajectory.metrics:\n",
        "        trajectory.metrics[\"hops\"] = hop_number\n",
        "\n",
        "    if op_client.api_key:\n",
        "        messages = trajectory.messages()\n",
        "        if messages[-1][\"role\"] == \"assistant\":\n",
        "            messages = messages[:-1]\n",
        "\n",
        "        try:\n",
        "            op_client.report(\n",
        "                requested_at=requested_at,\n",
        "                received_at=int(time.time() * 1000),\n",
        "                req_payload={\n",
        "                    \"model\": model.name,\n",
        "                    \"messages\": messages,\n",
        "                    \"metadata\": {\n",
        "                        \"notebook-id\": \"wikihop\",\n",
        "                        \"step\": str(scenario.step),\n",
        "                        \"final_hops\": str(hop_number),\n",
        "                        \"success\": str(trajectory.metrics[\"success\"]),\n",
        "                        \"reward\": str(trajectory.reward),\n",
        "                        \"target_url\": target_url,\n",
        "                        \"final_url\": current_url,\n",
        "                        \"error\": trajectory.metadata[\"error\"] if \"error\" in trajectory.metadata else \"none\",\n",
        "                    },\n",
        "                },\n",
        "                resp_payload=chat_completion,\n",
        "                status_code=200,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error reporting to OpenPipe: {e}\")\n",
        "\n",
        "    return trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if TEST_MODE:\n",
        "    await rollout(model, WikihopScenario(step=0, target_url=\"https://en.wikipedia.org/wiki/Particle_physics\"))\n",
        "    raise Exception(\"stopping early for test mode\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "The training loop is where the magic happens. For each of the 50 steps defined below, the rollout function will be called 48 times in parallel with different target Wikipedia pages. This means that 48 Wikipedia navigation tasks will be performed at once, each with a randomly selected target page.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will delete all but the most recent checkpoint and train the model on the new trajectories.\n",
        "\n",
        "Inference will be blocked until the training is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08579e4d8e7f415d8af4e0ab9e6fac3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import asyncio\n",
        "import judge_group\n",
        "import importlib\n",
        "\n",
        "# refresh judge_group.py\n",
        "importlib.reload(judge_group)\n",
        "\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "for i in range(await model.get_step(), 50):\n",
        "\n",
        "    batch_start_idx = i * batch_size % len(TARGET_URLS)\n",
        "    batch_end_idx = (i + 1) * batch_size % len(TARGET_URLS)\n",
        "\n",
        "    batch_urls = TARGET_URLS[batch_start_idx:batch_end_idx]\n",
        "\n",
        "    \n",
        "    train_groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, WikihopScenario(step=i, target_url=target_url)) for _ in range(8)\n",
        "            )\n",
        "            for target_url in batch_urls\n",
        "        ),\n",
        "        pbar_desc=\"gather\",\n",
        "    )\n",
        "    # judge simultaneously\n",
        "    judge_promises = []\n",
        "    for group in train_groups:\n",
        "        judge_promises.append(judge_group.judge_group(_model_name=model.name, trajectories=group.trajectories, debug=True))\n",
        "    await asyncio.gather(*judge_promises)\n",
        "\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(train_groups, config=art.TrainConfig(learning_rate=5e-5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Model\n",
        "\n",
        "Just like that, you've trained an agent to navigate Wikipedia! Now it's time to use your model outside of ART, in the wild! The easiest way to do that is to load it from disk, where it was saved after each training step, and either run inference on it locally or upload it to a central hub like HuggingFace.\n",
        "\n",
        "Check out the code below for a small demo of the model you just trained navigating Wikipedia to reach a target page!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading model from .art/tic-tac-toe-local/models/001-script/0100\n",
            "\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "move 1\n",
            "board:\n",
            "   1   2   3\n",
            "A  _ | _ | _\n",
            "B  _ | _ | _\n",
            "C  _ | _ | _\n",
            "\n",
            "agent move: <move>B1</move>\n",
            "updated board:\n",
            "   1   2   3\n",
            "A  _ | _ | _\n",
            "B  x | _ | _\n",
            "C  _ | _ | _\n",
            "\n",
            "\n",
            "move 3\n",
            "board:\n",
            "   1   2   3\n",
            "A  _ | _ | _\n",
            "B  x | _ | _\n",
            "C  _ | o | _\n",
            "\n",
            "agent move: <move>A1</move>\n",
            "updated board:\n",
            "   1   2   3\n",
            "A  x | _ | _\n",
            "B  x | _ | _\n",
            "C  _ | o | _\n",
            "\n",
            "\n",
            "move 5\n",
            "board:\n",
            "   1   2   3\n",
            "A  x | o | _\n",
            "B  x | _ | _\n",
            "C  _ | o | _\n",
            "\n",
            "agent move: <move>C1</move>\n",
            "updated board:\n",
            "   1   2   3\n",
            "A  x | o | _\n",
            "B  x | _ | _\n",
            "C  x | o | _\n",
            "\n",
            "game finished in 5 moves\n",
            "game won! üí™\n",
            "final board:\n",
            "\n",
            "   1   2   3\n",
            "A  x | o | _\n",
            "B  x | _ | _\n",
            "C  x | o | _\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "# example: .art/wikihop-navigation/models/001-wikihop/0003\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "print(f\"loading model from {lora_model_path}\\n\")\n",
        "\n",
        "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=lora_model_path,\n",
        "    max_seq_length=16384,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(peft_model)\n",
        "\n",
        "# Demo navigation to a target page\n",
        "current_url = STARTING_URL\n",
        "target_url = random.choice(TARGET_URLS)\n",
        "max_hops = 5\n",
        "hop_number = 0\n",
        "\n",
        "print(f\"üéØ Target: {target_url}\")\n",
        "print(f\"üöÄ Starting from: {current_url}\\n\")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"You are a Wikipedia navigator. Your goal is to reach the target page: {target_url}\\n\\nYou will be shown the first paragraph of each Wikipedia page. Select the link that is most likely to get you closer to your target. If you see a direct link to your target, choose it immediately.\\n\\nRespond with ONLY the full URL in this exact format: <url>https://en.wikipedia.org/wiki/YourChoice</url>\",\n",
        "    },\n",
        "]\n",
        "\n",
        "while hop_number < max_hops:\n",
        "    # Check if we've reached the target\n",
        "    if check_target_match(current_url, target_url):\n",
        "        print(f\"üéâ SUCCESS! Reached target in {hop_number} hops!\")\n",
        "        break\n",
        "        \n",
        "    # Read the first paragraph of the current page\n",
        "    try:\n",
        "        first_paragraph = read_first_paragraphs_with_links(current_url)\n",
        "        if first_paragraph.startswith(\"Error\"):\n",
        "            print(f\"‚ùå Error reading page: {first_paragraph}\")\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Exception reading page: {e}\")\n",
        "        break\n",
        "\n",
        "    # Show current page\n",
        "    page_title = current_url.split('/wiki/')[-1].replace('_', ' ')\n",
        "    print(f\"üìñ Hop {hop_number + 1}: Currently on '{page_title}'\")\n",
        "    \n",
        "    # Extract and show available links\n",
        "    links = extract_urls(first_paragraph)\n",
        "    print(f\"üîó Found {len(links)} links: {', '.join([link.split(' -> ')[0] for link in links[:5]])}{'...' if len(links) > 5 else ''}\")\n",
        "    \n",
        "    # Add the current page content to the trajectory\n",
        "    page_content = f\"Current page: {page_title}\\nTarget: {target_url}\\nHop {hop_number + 1}/{max_hops}\\n\\nFirst paragraph:\\n{first_paragraph}\"\n",
        "    messages.append({\"role\": \"user\", \"content\": page_content})\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    def get_completion() -> str:\n",
        "        with torch.no_grad():\n",
        "            outputs = peft_model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "            )\n",
        "            return tokenizer.decode(\n",
        "                outputs[0][inputs.shape[1] :], skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        content = get_completion()\n",
        "        print(f\"ü§ñ Model response: {content}\")\n",
        "        \n",
        "        # Parse the URL from the response\n",
        "        try:\n",
        "            root = ET.fromstring(content)\n",
        "            next_url = root.text\n",
        "            if not next_url:\n",
        "                raise ValueError(\"Empty URL in response\")\n",
        "            next_url = next_url.strip()\n",
        "        except ET.ParseError:\n",
        "            # Try to extract URL with regex as fallback\n",
        "            import re\n",
        "            url_pattern = r'https://en\\.wikipedia\\.org/wiki/[^\\s<>]+'\n",
        "            matches = re.findall(url_pattern, content)\n",
        "            if matches:\n",
        "                next_url = matches[0]\n",
        "            else:\n",
        "                print(\"‚ùå Could not parse URL from response\")\n",
        "                break\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating completion: {e}\")\n",
        "        break\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "\n",
        "    # Show the selected link\n",
        "    next_title = next_url.split('/wiki/')[-1].replace('_', ' ')\n",
        "    print(f\"üîó Selected: '{next_title}' -> {next_url}\")\n",
        "    \n",
        "    # Move to the next page\n",
        "    current_url = next_url\n",
        "    hop_number += 1\n",
        "    print()\n",
        "\n",
        "# Final result\n",
        "if hop_number >= max_hops and not check_target_match(current_url, target_url):\n",
        "    print(f\"‚è∞ Reached maximum hops ({max_hops}) without finding target\")\n",
        "    final_title = current_url.split('/wiki/')[-1].replace('_', ' ')\n",
        "    print(f\"üìç Final page: '{final_title}'\")\n",
        "elif check_target_match(current_url, target_url):\n",
        "    print(f\"üéâ SUCCESS! Found target '{target_url}' in {hop_number} hops!\")\n",
        "else:\n",
        "    print(f\"‚ùå Navigation stopped due to error\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
